{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkaAYkLrF9PhHCmX4Nq7H/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Odima-dev/Data-Science-and-Machine-Learning/blob/main/SimpleConv2d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Problem 1:Creating a 2-D convolutional layer\n",
        "class Conv2d:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, learning_rate=0.01):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kh, self.kw = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        # Xavier Initialization\n",
        "        scale = np.sqrt(1. / (in_channels * self.kh * self.kw))\n",
        "        self.W = np.random.randn(out_channels, in_channels, self.kh, self.kw) * scale\n",
        "        self.b = np.zeros(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = (H + 2 * self.padding - self.kh) // self.stride + 1\n",
        "        out_w = (W + 2 * self.padding - self.kw) // self.stride + 1\n",
        "        self.out_shape = (N, self.out_channels, out_h, out_w)\n",
        "\n",
        "        if self.padding > 0:\n",
        "            x = np.pad(x, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
        "\n",
        "        self.x_padded = x\n",
        "        out = np.zeros((N, self.out_channels, out_h, out_w))\n",
        "\n",
        "        for n in range(N):\n",
        "            for oc in range(self.out_channels):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride\n",
        "                        h_end = h_start + self.kh\n",
        "                        w_start = j * self.stride\n",
        "                        w_end = w_start + self.kw\n",
        "                        region = x[n, :, h_start:h_end, w_start:w_end]\n",
        "                        out[n, oc, i, j] = np.sum(region * self.W[oc]) + self.b[oc]\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, C, H, W = self.x.shape\n",
        "        dx = np.zeros_like(self.x_padded, dtype=np.float32)\n",
        "        dW = np.zeros_like(self.W, dtype=np.float32)\n",
        "        db = np.zeros_like(self.b, dtype=np.float32)\n",
        "\n",
        "\n",
        "        _, _, out_h, out_w = dout.shape\n",
        "\n",
        "        for n in range(N):\n",
        "            for oc in range(self.out_channels):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride\n",
        "                        h_end = h_start + self.kh\n",
        "                        w_start = j * self.stride\n",
        "                        w_end = w_start + self.kw\n",
        "                        region = self.x_padded[n, :, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                        dW[oc] += region * dout[n, oc, i, j]\n",
        "                        db[oc] += dout[n, oc, i, j]\n",
        "                        dx[n, :, h_start:h_end, w_start:w_end] += self.W[oc] * dout[n, oc, i, j]\n",
        "\n",
        "        # Removing padding if added\n",
        "        if self.padding > 0:\n",
        "            dx = dx[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
        "\n",
        "        # Updating weights\n",
        "        self.W -= self.lr * dW\n",
        "        self.b -= self.lr * db\n",
        "\n",
        "        return dx\n",
        "\n",
        "# Problem 2: Experiments with 2D convolutional layers on small arrays\n",
        "\n",
        "# Input data when flowing CNN2 forwards (1, 1, 4, 4)\n",
        "x = np.array([[[[1, 2, 3, 4],\n",
        "                [5, 6, 7, 8],\n",
        "                [9, 10, 11, 12],\n",
        "                [13, 14, 15, 16]]]])\n",
        "\n",
        "# Manually setting filters\n",
        "w = np.array([\n",
        "    [[[0, 0, 0], [0, 1, 0], [0, -1, 0]]],\n",
        "    [[[0, 0, 0], [0, -1, 1], [0, 0, 0]]]\n",
        "]).astype(np.float32)  # Cast to float\n",
        "\n",
        "b = np.array([0, 0], dtype=np.float32)\n",
        "\n",
        "# Conv2d with 1 input channel, 2 output channels, kernel 3x3\n",
        "conv = Conv2d(in_channels=1, out_channels=2, kernel_size=(3, 3), stride=1, padding=0)\n",
        "conv.W = w.copy()\n",
        "conv.b = b.copy()\n",
        "\n",
        "# Forward pass\n",
        "out = conv.forward(x)\n",
        "print(\"Forward Output:\\n\", out)\n",
        "\n",
        "# Backward test\n",
        "dout = np.array([[[[-4, -4], [-4, -4]],\n",
        "                  [[1, -7], [1, -11]]]], dtype=np.float32)\n",
        "dx = conv.backward(dout)\n",
        "print(\"\\nBackward Output (dX):\\n\", dx)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Problem 3: Output size after 2-dimensional convolution\n",
        "def conv2d_output_size(H_in, W_in, kernel_size, stride=1, padding=0):\n",
        "    kh, kw = kernel_size\n",
        "    H_out = (H_in + 2 * padding - kh) // stride + 1\n",
        "    W_out = (W_in + 2 * padding - kw) // stride + 1\n",
        "    return H_out, W_out\n",
        "\n",
        "\n",
        "# Problem 4: Creation of maximum pooling layer\n",
        "class MaxPool2D:\n",
        "    def __init__(self, pool_size=(2, 2), stride=2):\n",
        "        self.ph, self.pw = pool_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = (H - self.ph) // self.stride + 1\n",
        "        out_w = (W - self.pw) // self.stride + 1\n",
        "        self.arg_max = np.zeros((N, C, out_h, out_w), dtype=np.int32)\n",
        "\n",
        "        out = np.zeros((N, C, out_h, out_w))\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride\n",
        "                        w_start = j * self.stride\n",
        "                        window = x[n, c, h_start:h_start+self.ph, w_start:w_start+self.pw]\n",
        "                        out[n, c, i, j] = np.max(window)\n",
        "                        self.arg_max[n, c, i, j] = np.argmax(window)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, C, H, W = self.x.shape\n",
        "        out_h, out_w = dout.shape[2:]\n",
        "        dx = np.zeros_like(self.x)\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride\n",
        "                        w_start = j * self.stride\n",
        "                        index = self.arg_max[n, c, i, j]\n",
        "                        h_index = h_start + index // self.pw\n",
        "                        w_index = w_start + index % self.pw\n",
        "                        dx[n, c, h_index, w_index] += dout[n, c, i, j]\n",
        "        return dx\n",
        "\n",
        "# Problem 5: (Advance task) Creating average pooling\n",
        "class AveragePool2D:\n",
        "    def __init__(self, pool_size=(2, 2), stride=2):\n",
        "        self.ph, self.pw = pool_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = (H - self.ph) // self.stride + 1\n",
        "        out_w = (W - self.pw) // self.stride + 1\n",
        "\n",
        "        out = np.zeros((N, C, out_h, out_w))\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride\n",
        "                        w_start = j * self.stride\n",
        "                        window = x[n, c, h_start:h_start+self.ph, w_start:w_start+self.pw]\n",
        "                        out[n, c, i, j] = np.mean(window)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, C, H, W = self.x.shape\n",
        "        out_h, out_w = dout.shape[2:]\n",
        "        dx = np.zeros_like(self.x)\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride\n",
        "                        w_start = j * self.stride\n",
        "                        dx[n, c, h_start:h_start+self.ph, w_start:w_start+self.pw] += dout[n, c, i, j] / (self.ph * self.pw)\n",
        "        return dx\n",
        "\n",
        "\n",
        "# Problem 6: Smoothing\n",
        "class Flatten:\n",
        "    def forward(self, x):\n",
        "        self.orig_shape = x.shape\n",
        "        return x.reshape(x.shape[0], -1)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout.reshape(self.orig_shape)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Problem 7: Learning and estimation\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, x):\n",
        "        self.mask = (x > 0)\n",
        "        return x * self.mask\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "class Dense:\n",
        "    def __init__(self, in_features, out_features, lr=0.01):\n",
        "        scale = np.sqrt(1. / in_features)\n",
        "        self.W = np.random.randn(in_features, out_features) * scale\n",
        "        self.b = np.zeros(out_features)\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.dot(x, self.W) + self.b\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        db = np.sum(dout, axis=0)\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.W -= self.lr * dW\n",
        "        self.b -= self.lr * db\n",
        "        return dx\n",
        "\n",
        "class SoftmaxCrossEntropy:\n",
        "    def forward(self, x, y):\n",
        "        self.y = y\n",
        "        self.y_pred = self._softmax(x)\n",
        "        return self._cross_entropy(self.y_pred, y)\n",
        "\n",
        "    def backward(self):\n",
        "        return (self.y_pred - self.y) / self.y.shape[0]\n",
        "\n",
        "    def _softmax(self, x):\n",
        "        x = x - np.max(x, axis=1, keepdims=True)\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "\n",
        "    def _cross_entropy(self, y_pred, y_true):\n",
        "        return -np.sum(y_true * np.log(y_pred + 1e-7)) / y_true.shape[0]\n",
        "\n",
        "# Preprocess MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train[:1000].astype(np.float32) / 255.0\n",
        "x_test = x_test[:200].astype(np.float32) / 255.0\n",
        "y_train = to_categorical(y_train[:1000], 10)\n",
        "y_test = to_categorical(y_test[:200], 10)\n",
        "\n",
        "x_train = x_train.reshape(-1, 1, 28, 28)\n",
        "x_test = x_test.reshape(-1, 1, 28, 28)\n",
        "\n",
        "# Define simple CNN\n",
        "class Scratch2dCNNClassifier:\n",
        "    def __init__(self):\n",
        "        self.conv = Conv2d(1, 8, (3, 3), stride=1, padding=1)\n",
        "        self.relu1 = ReLU()\n",
        "        self.pool = MaxPool2D(pool_size=(2, 2), stride=2)\n",
        "        self.flatten = Flatten()\n",
        "        self.fc1 = Dense(14*14*8, 64)\n",
        "        self.relu2 = ReLU()\n",
        "        self.fc2 = Dense(64, 10)\n",
        "        self.loss_fn = SoftmaxCrossEntropy()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv.forward(x)\n",
        "        x = self.relu1.forward(x)\n",
        "        x = self.pool.forward(x)\n",
        "        x = self.flatten.forward(x)\n",
        "        x = self.fc1.forward(x)\n",
        "        x = self.relu2.forward(x)\n",
        "        x = self.fc2.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = self.fc2.backward(dout)\n",
        "        dout = self.relu2.backward(dout)\n",
        "        dout = self.fc1.backward(dout)\n",
        "        dout = self.flatten.backward(dout)\n",
        "        dout = self.pool.backward(dout)\n",
        "        dout = self.relu1.backward(dout)\n",
        "        dout = self.conv.backward(dout)\n",
        "\n",
        "    def train(self, x, y):\n",
        "        out = self.forward(x)\n",
        "        loss = self.loss_fn.forward(out, y)\n",
        "        dout = self.loss_fn.backward()\n",
        "        self.backward(dout)\n",
        "        return loss\n",
        "\n",
        "    def predict(self, x):\n",
        "        out = self.forward(x)\n",
        "        return np.argmax(out, axis=1)\n",
        "\n",
        "# Training\n",
        "model = Scratch2dCNNClassifier()\n",
        "epochs = 3\n",
        "batch_size = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss_sum = 0\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        x_batch = x_train[i:i+batch_size]\n",
        "        y_batch = y_train[i:i+batch_size]\n",
        "        loss = model.train(x_batch, y_batch)\n",
        "        loss_sum += loss\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss_sum:.4f}\")\n",
        "\n",
        "# Accuracy\n",
        "preds = model.predict(x_test)\n",
        "true = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(preds == true)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Problem 8: (Advance assignment) LeNet\n",
        "class LeNet:\n",
        "    def __init__(self):\n",
        "        self.conv1 = Conv2d(1, 6, (5, 5), stride=1, padding=0)\n",
        "        self.relu1 = ReLU()\n",
        "        self.pool1 = MaxPool2D(pool_size=(2, 2), stride=2)\n",
        "        self.conv2 = Conv2d(6, 16, (5, 5), stride=1, padding=0)\n",
        "        self.relu2 = ReLU()\n",
        "        self.pool2 = MaxPool2D(pool_size=(2, 2), stride=2)\n",
        "        self.flatten = Flatten()\n",
        "        self.fc1 = Dense(16 * 4 * 4, 120)\n",
        "        self.relu3 = ReLU()\n",
        "        self.fc2 = Dense(120, 84)\n",
        "        self.relu4 = ReLU()\n",
        "        self.fc3 = Dense(84, 10)\n",
        "        self.loss_fn = SoftmaxCrossEntropy()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1.forward(x)\n",
        "        x = self.relu1.forward(x)\n",
        "        x = self.pool1.forward(x)\n",
        "        x = self.conv2.forward(x)\n",
        "        x = self.relu2.forward(x)\n",
        "        x = self.pool2.forward(x)\n",
        "        x = self.flatten.forward(x)\n",
        "        x = self.fc1.forward(x)\n",
        "        x = self.relu3.forward(x)\n",
        "        x = self.fc2.forward(x)\n",
        "        x = self.relu4.forward(x)\n",
        "        x = self.fc3.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = self.fc3.backward(dout)\n",
        "        dout = self.relu4.backward(dout)\n",
        "        dout = self.fc2.backward(dout)\n",
        "        dout = self.relu3.backward(dout)\n",
        "        dout = self.fc1.backward(dout)\n",
        "        dout = self.flatten.backward(dout)\n",
        "        dout = self.pool2.backward(dout)\n",
        "        dout = self.relu2.backward(dout)\n",
        "        dout = self.conv2.backward(dout)\n",
        "        dout = self.pool1.backward(dout)\n",
        "        dout = self.relu1.backward(dout)\n",
        "        dout = self.conv1.backward(dout)\n",
        "\n",
        "    def train(self, x, y):\n",
        "        out = self.forward(x)\n",
        "        loss = self.loss_fn.forward(out, y)\n",
        "        dout = self.loss_fn.backward()\n",
        "        self.backward(dout)\n",
        "        return loss\n",
        "\n",
        "    def predict(self, x):\n",
        "        out = self.forward(x)\n",
        "        return np.argmax(out, axis=1)\n",
        "\n",
        "# Training LeNet\n",
        "lenet = LeNet()\n",
        "for epoch in range(3):\n",
        "    loss_sum = 0\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        x_batch = x_train[i:i+batch_size]\n",
        "        y_batch = y_train[i:i+batch_size]\n",
        "        loss = lenet.train(x_batch, y_batch)\n",
        "        loss_sum += loss\n",
        "    print(f\"[LeNet] Epoch {epoch+1}, Loss: {loss_sum:.4f}\")\n",
        "\n",
        "preds = lenet.predict(x_test)\n",
        "true = np.argmax(y_test, axis=1)\n",
        "acc = np.mean(preds == true)\n",
        "print(\"[LeNet] Test Accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNd-SchuctmI",
        "outputId": "e9a14867-e58a-45a4-9eb4-2694222c0c27"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward Output:\n",
            " [[[[-4. -4.]\n",
            "   [-4. -4.]]\n",
            "\n",
            "  [[ 1.  1.]\n",
            "   [ 1.  1.]]]]\n",
            "\n",
            "Backward Output (dX):\n",
            " [[[[  0.   0.   0.   0.]\n",
            "   [  0.  -5.   4.  -7.]\n",
            "   [  0.  -1.  12. -11.]\n",
            "   [  0.   4.   4.   0.]]]]\n",
            "\n",
            "\n",
            "Epoch 1, Loss: 22.9770\n",
            "Epoch 2, Loss: 22.0444\n",
            "Epoch 3, Loss: 21.1555\n",
            "Test Accuracy: 0.42\n",
            "\n",
            "\n",
            "[LeNet] Epoch 1, Loss: 23.1000\n",
            "[LeNet] Epoch 2, Loss: 22.8708\n",
            "[LeNet] Epoch 3, Loss: 22.6752\n",
            "[LeNet] Test Accuracy: 0.145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#Problem 9: (Advance assignment) Survey of famous image recognition models**\n",
        "\n",
        "**AlexNet (2012)**\n",
        "\n",
        "AlexNet can be considered as one of the first deep convolutional neural networks that has achieved significant breakthrough in the domain of the computer vision and contributed to making deep learning popular. It has five convolutional layers then three fully connected layers. AlexNet had some of the following innovations. First, it replaced traditional sigmoid or tanh functions with ReLU ( Rectified Linear Unit ) and that allowed it to speed up the training process and bring better results. It also integrated dropout, regularization to minimize overfitting. It was trained by GPU on with data parallelism, and larger models could be trained. It also used Local Response Normalization (LRN) technique, but this is seldom applied in the contemporary architectures. The input of AlexNet is RGB images of dimension 224224.\n",
        "\n",
        "**VGG16 (2014)**\n",
        "\n",
        "VGG16 put forward deeper architecture involving a network than the AlexNet, entailing 13 convolutional layers and three fully connected layers- which total up to 16 learnable layers. The main contribution of the VGG16 training process is that the small size (3*3) convolutional filters are used across the network without variation. This strategy proved that an aggregate or series of small filters is capable of a higher performance than 7x7 filters. The network has a plain and homogeneous structure and this helps the creation as well as the expansion of the network. To bring about spatial dimensionality reduction, the insertion of Max pooling layers at the end of some convolutional blocks is done. Though VGG16 met better accuracy and depth unlike AlexNet, it is also slow and consumes more memory because it has more parameters than AlexNet."
      ],
      "metadata": {
        "id": "KYP1XsO8dlrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 10: Calculation of output size and number of parameters\n",
        "def compute_conv_output_and_params(H_in, W_in, C_in, kernel_size, C_out, stride=1, padding=0):\n",
        "    kh, kw = kernel_size\n",
        "\n",
        "    # Output dimensions\n",
        "    H_out = (H_in + 2 * padding - kh) // stride + 1\n",
        "    W_out = (W_in + 2 * padding - kw) // stride + 1\n",
        "\n",
        "    # Parameters per filter: C_in * kh * kw, plus 1 bias per output channel\n",
        "    params_per_filter = C_in * kh * kw + 1\n",
        "    total_params = params_per_filter * C_out\n",
        "\n",
        "    return (H_out, W_out, C_out), total_params\n",
        "\n",
        "\n",
        "# 1. Input: 144×144×3, Filter: 3×3, 6 filters, stride=1, padding=0\n",
        "out1, params1 = compute_conv_output_and_params(144, 144, 3, (3, 3), 6)\n",
        "print(\"Layer 1 Output:\", out1, \"Params:\", params1)\n",
        "\n",
        "# 2. Input: 60×60×24, Filter: 3×3, 48 filters, stride=1, padding=0\n",
        "out2, params2 = compute_conv_output_and_params(60, 60, 24, (3, 3), 48)\n",
        "print(\"Layer 2 Output:\", out2, \"Params:\", params2)\n",
        "\n",
        "# 3. Input: 20×20×10, Filter: 3×3, 20 filters, stride=2, padding=0\n",
        "out3, params3 = compute_conv_output_and_params(20, 20, 10, (3, 3), 20, stride=2)\n",
        "print(\"Layer 3 Output:\", out3, \"Params:\", params3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBhHarQPfVit",
        "outputId": "fe40a680-aa39-4b3b-f3ba-ecd2d468cc24"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 Output: (142, 142, 6) Params: 168\n",
            "Layer 2 Output: (58, 58, 48) Params: 10416\n",
            "Layer 3 Output: (9, 9, 20) Params: 1820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#Problem 11: (Advance assignment) Survey on filter size**\n",
        "\n",
        "**Why 3×3 filters are commonly used instead of larger ones such as 7×7**\n",
        "\n",
        "The use smaller-sized convolutional filters, e.g., 3x3 rather than large convolutional filters, e.g., 7x7, is a common convention of the recent CNN architectures due to multiple reasons. First, residual stacking with several layers composed of 3 x 3 is more non-linear to the network. The activation functions occur after each layer, thus with three 3x3 layers more activation operations can be performed than with one 7x7 layer, and in this way deeper and expressive networks are possible. Secondly, 3x3 filters have a better usage of parameters. As an example, that 7 x 7 convolutional layer with input and output channels, C would have 49C 2 parameters, but with three convolutional layers of three pixels in size stacked on top of each other, we need only 27C 2 parameters a little more than half. The three 3 3 layers were keeping only 7 x 7 fields of influence despite the reduced parameter number effectively, the network would be able to comprehend the same spatial information with the benefit of marginally more learning capabilities and less computations.\n",
        "\n",
        "**The effect of a 1 x 1 filter with no height or width direction**\n",
        "\n",
        "This onvolution filter fails to extract spatial information because height has a value of one and also the width. Rather, it acts on the channel dimension instead and has the effect of instantaneously performing a fully connected layer on every pixel position in the image. Primary applications of 1x1 filters are: reducing dimensionality of the input, e.g. reducing the number of channels prior to undergoing a more expensive computation-wise convolution and embedding the network with additional depth without expanding it spatially. Further, learning non-linear combination in the feature channels can be done using 1 by 1 convolutions. The idea was conceived in the Network-in-Network (NIN) architecture and it has been successfully implemented by GoogLeNet (Inception), in which it was instrumental to reduce the depth of the networks and improve efficiency.\n"
      ],
      "metadata": {
        "id": "LAdBnOEtftmp"
      }
    }
  ]
}