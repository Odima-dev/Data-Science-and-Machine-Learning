{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjUDRrB8b3dtDUx4AndStx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Odima-dev/Data-Science-and-Machine-Learning/blob/main/Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Creating my project folder\n",
        "!mkdir -p \"/content/drive/MyDrive/Seq2SeqProject\"\n",
        "project_folder = \"/content/drive/MyDrive/Seq2SeqProject\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apU_n9dhFLCI",
        "outputId": "e5e7675e-2d0f-42d6-84fa-c5190012aa74"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Seq2SeqProject"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMUdGZ6_GSqY",
        "outputId": "93fc4fcc-ac09-401d-a022-97e01f3c9d87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Seq2SeqProject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 1: Execution of machine translation and code reading\n",
        "\n",
        "#Running the Code\n",
        "\"\"\"\n",
        "Title: Character-level recurrent sequence-to-sequence model\n",
        "Author: [fchollet](https://twitter.com/fchollet)\n",
        "Date created: 2017/09/29\n",
        "Last modified: 2023/11/22\n",
        "Description: Character-level recurrent sequence-to-sequence model.\n",
        "Accelerator: GPU\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "## Introduction\n",
        "\n",
        "This example demonstrates how to implement a basic character-level\n",
        "recurrent sequence-to-sequence model. We apply it to translating\n",
        "short English sentences into short French sentences,\n",
        "character-by-character. Note that it is fairly unusual to\n",
        "do character-level machine translation, as word-level\n",
        "models are more common in this domain.\n",
        "\n",
        "**Summary of the algorithm**\n",
        "\n",
        "- We start with input sequences from a domain (e.g. English sentences)\n",
        "    and corresponding target sequences from another domain\n",
        "    (e.g. French sentences).\n",
        "- An encoder LSTM turns input sequences to 2 state vectors\n",
        "    (we keep the last LSTM state and discard the outputs).\n",
        "- A decoder LSTM is trained to turn the target sequences into\n",
        "    the same sequence but offset by one timestep in the future,\n",
        "    a training process called \"teacher forcing\" in this context.\n",
        "    It uses as initial state the state vectors from the encoder.\n",
        "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
        "    given `targets[...t]`, conditioned on the input sequence.\n",
        "- In inference mode, when we want to decode unknown input sequences, we:\n",
        "    - Encode the input sequence into state vectors\n",
        "    - Start with a target sequence of size 1\n",
        "        (just the start-of-sequence character)\n",
        "    - Feed the state vectors and 1-char target sequence\n",
        "        to the decoder to produce predictions for the next character\n",
        "    - Sample the next character using these predictions\n",
        "        (we simply use argmax).\n",
        "    - Append the sampled character to the target sequence\n",
        "    - Repeat until we generate the end-of-sequence character or we\n",
        "        hit the character limit.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "## Setup\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "\"\"\"\n",
        "## Download the data\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "data_path = \"fra.txt\"\n",
        "\n",
        "\"\"\"\n",
        "## Configuration\n",
        "\"\"\"\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 5  # I adjusted the number of epochs to 5 instead of 100 so as to reduce training time that the process would take.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = \"fra.txt\"\n",
        "\n",
        "\"\"\"\n",
        "## Prepare the data\n",
        "\"\"\"\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "\"\"\"\n",
        "## Build the model\n",
        "\"\"\"\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Setting up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "\"\"\"\n",
        "## Train the model\n",
        "\"\"\"\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "# Save model\n",
        "model.save(\"s2s_model.keras\")\n",
        "\n",
        "\"\"\"\n",
        "## Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n",
        "\"\"\"\n",
        "\n",
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(\"s2s_model.keras\")\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "You can now generate decoded sentences as such:\n",
        "\"\"\"\n",
        "\n",
        "for seq_index in range(20):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EFdKhAdTHIVT",
        "outputId": "564347e7-0e8f-42bd-f0db-328ff1e6a126"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3770a329-0b48-453a-a544-966d93304e0f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3770a329-0b48-453a-a544-966d93304e0f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving fra.txt to fra.txt\n",
            "Number of samples: 10000\n",
            "Number of unique input tokens: 70\n",
            "Number of unique output tokens: 91\n",
            "Max sequence length for inputs: 14\n",
            "Max sequence length for outputs: 59\n",
            "Epoch 1/5\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 404ms/step - accuracy: 0.7064 - loss: 1.5486 - val_accuracy: 0.7166 - val_loss: 1.1665\n",
            "Epoch 2/5\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 419ms/step - accuracy: 0.7463 - loss: 0.9671 - val_accuracy: 0.7214 - val_loss: 0.9667\n",
            "Epoch 3/5\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 399ms/step - accuracy: 0.7631 - loss: 0.8646 - val_accuracy: 0.7492 - val_loss: 0.8769\n",
            "Epoch 4/5\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 385ms/step - accuracy: 0.7841 - loss: 0.7773 - val_accuracy: 0.7718 - val_loss: 0.7831\n",
            "Epoch 5/5\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 396ms/step - accuracy: 0.8041 - loss: 0.6852 - val_accuracy: 0.7897 - val_loss: 0.7316\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Tom s mont pais !\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Tom s mont pais !\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Tom s mont pais !\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Tom s mont pais !\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Tom s mous pais !\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Tom s mous pais !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Toms s mont !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Toms s mont !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Toms s mont !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Toms s mont !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Toms s mont !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Toms s mont !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Toms s mont !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Toms s mont !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Toms s mont pais !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Toms s mont pais !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Toms s mont pais !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Toms s mont pais !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Toms s mont pais !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Toms s mont pais !\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarizing What Each Part of This Sample Code Does**\n",
        "\n",
        "This code essentially trains and creates a character level sequence to sequence (Seq2Seq) neural network that translates short English sentences into French sentences using Keras and LSTM (Long Short-Term Memory) layers. And so here goes an explanation of the lines in a nutshell:\n",
        "\n",
        "\n",
        "\n",
        "* Lines 1–17: Metadata and Introduction the code author, creation date, and a high-level explanation Seq2Seq translation.\n",
        "* Lines 51–56: Import Libraries\n",
        "* Lines 59–62: Downloading the Data an Defining Directory Path\n",
        "* Lines 65–70: Configuring Hyperparameter\n",
        "* Lines 73–113: Data Preparation & Tokenization\n",
        "* Lines 115–140: Performing One-Hot Encoding\n",
        "* Lines 143–167: Building the Model\n",
        "* Lines 170-184: Training and Compiling\n",
        "* Lines 187–223: Inference Model Setup\n",
        "* Lines 226–260: Decoding Sequence Function\n",
        "* Lines 267–274: Generating Sample Translations\n",
        "\n"
      ],
      "metadata": {
        "id": "RBtehN--Iyuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2: Execution of a trained model of image captioning\n",
        "# I will use a Pre-trained PyTorch Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import os\n",
        "import requests\n",
        "import urllib.request\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import argparse\n",
        "import zipfile\n",
        "\n",
        "# Installing required packages\n",
        "!pip install torch torchvision pillow requests\n",
        "\n",
        "# Uploading the already trained model files manually\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Unzipping vocab.zip\n",
        "with zipfile.ZipFile(\"vocap.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# Unzipping pretrained_model.zip\n",
        "with zipfile.ZipFile(\"pretrained_model.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "required_files = ['vocab.pkl', 'encoder-5-3000.pkl', 'decoder-5-3000.pkl']\n",
        "for f in required_files:\n",
        "    print(f\"{f}: {'File Exists!' if os.path.exists(f) else 'File Missing'}\")\n",
        "\n",
        "# Defining the model architectures (from the original repository)\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]      # deleting the last fc layer.\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        features = features.reshape(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
        "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def forward(self, features, captions, lengths):\n",
        "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
        "        hiddens, _ = self.lstm(packed)\n",
        "        outputs = self.linear(hiddens[0])\n",
        "        return outputs\n",
        "\n",
        "    def sample(self, features, states=None):\n",
        "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        for i in range(self.max_seq_length):\n",
        "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
        "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
        "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
        "            sampled_ids.append(predicted)\n",
        "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
        "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
        "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
        "        return sampled_ids\n",
        "\n",
        "def load_image(image_path, transform=None):\n",
        "    \"\"\"Load and preprocess image\"\"\"\n",
        "    if image_path.startswith('http'):\n",
        "        response = requests.get(image_path)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    if transform is not None:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Minimal Vocabulary class definition to support unpickling\n",
        "class Vocabulary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        return self.word2idx.get(word, self.word2idx.get('<unk>', 0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "    def items(self):\n",
        "        return self.word2idx.items()\n",
        "\n",
        "def load_models_and_vocab():\n",
        "    \"\"\"Load the pre-trained models and vocabulary\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Loading vocabulary\n",
        "    with open('vocab.pkl', 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "\n",
        "    # Model parameters\n",
        "    embed_size = 256\n",
        "    hidden_size = 512\n",
        "    num_layers = 1\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Building models\n",
        "    encoder = EncoderCNN(embed_size).eval()\n",
        "    decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    # Loading pre-trained weights\n",
        "    encoder_path = 'encoder-5-3000.pkl'\n",
        "    decoder_path = 'decoder-5-3000.pkl'\n",
        "\n",
        "    # Loading encoder weights\n",
        "    encoder.load_state_dict(torch.load(encoder_path, map_location=device))\n",
        "    print(f\"✓ Loaded encoder weights from {encoder_path}\")\n",
        "\n",
        "    # Loading decoder weights\n",
        "    decoder.load_state_dict(torch.load(decoder_path, map_location=device))\n",
        "    print(f\"✓ Loaded decoder weights from {decoder_path}\")\n",
        "\n",
        "    # Moving models to device\n",
        "    encoder = encoder.to(device)\n",
        "    decoder = decoder.to(device)\n",
        "\n",
        "    return encoder, decoder, vocab, device\n",
        "\n",
        "def generate_caption(image_path, encoder, decoder, vocab, transform, device):\n",
        "    \"\"\"Generate caption for an image using the pre-trained model\"\"\"\n",
        "    # Loading and preprocessing image\n",
        "    image = load_image(image_path, transform)\n",
        "    image_tensor = image.to(device)\n",
        "\n",
        "    # Generating caption\n",
        "    with torch.no_grad():\n",
        "        feature = encoder(image_tensor)\n",
        "        sampled_ids = decoder.sample(feature)\n",
        "        sampled_ids = sampled_ids[0].cpu().numpy()\n",
        "\n",
        "    # Converting word_ids to words\n",
        "    vocab_inv = {v: k for k, v in vocab.items()}\n",
        "    sampled_caption = []\n",
        "    for word_id in sampled_ids:\n",
        "        word = vocab_inv[word_id]\n",
        "        sampled_caption.append(word)\n",
        "        if word == '<end>':\n",
        "            break\n",
        "\n",
        "    sentence = ' '.join(sampled_caption)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"Image Captioning with Pre-trained PyTorch Model\")\n",
        "\n",
        "    # Loading models and vocabulary\n",
        "    try:\n",
        "        encoder, decoder, vocab, device = load_models_and_vocab()\n",
        "        print(f\"✓ Models loaded successfully on {device}\")\n",
        "        print(f\"✓ Vocabulary size: {len(vocab)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models: {e}\")\n",
        "        return\n",
        "\n",
        "    # Image preprocessing transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                           (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    # Testing with online images\n",
        "\n",
        "    test_images = [\n",
        "        'https://images.unsplash.com/photo-1552053831-71594a27632d?w=400',      # a dog with a flower\n",
        "        'https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400',   # cat\n",
        "        'https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=400',   # A man\n",
        "        'https://images.unsplash.com/photo-1549298916-b41d501d3772?w=400',      # A Nikey Shoe\n",
        "        'https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400'    # Mountains\n",
        "    ]\n",
        "\n",
        "    print(\"\\\\nGenerating captions for test images:\")\n",
        "\n",
        "\n",
        "    for i, image_path in enumerate(test_images):\n",
        "        try:\n",
        "            print(f\"\\\\nProcessing Image {i+1}...\")\n",
        "            print(f\"URL: {image_path}\")\n",
        "\n",
        "            caption = generate_caption(image_path, encoder, decoder, vocab, transform, device)\n",
        "            print(f\"Generated Caption: {caption}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {i+1}: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Now I will test with my own uploaded images\n",
        "def test_with_uploaded_images():\n",
        "\n",
        "    #Loading images\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Loading models\n",
        "    encoder, decoder, vocab, device = load_models_and_vocab()\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                           (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    # Replacing with my uploaded image paths\n",
        "    uploaded_images = [\n",
        "        '/content/drive/MyDrive/Seq2SeqProject/pexels-laker-6156582.jpg',              # A Bus\n",
        "        '/content/drive/MyDrive/Seq2SeqProject/pexels-dariuskrs-2253938.jpg',          # Traffic Light\n",
        "        '/content/drive/MyDrive/Seq2SeqProject/pexels-jamphotography-2626665.jpg',     # Motorcycle\n",
        "    ]\n",
        "\n",
        "    print(\"Testing with my own uploaded images:\")\n",
        "\n",
        "    for i, image_path in enumerate(uploaded_images):\n",
        "        if os.path.exists(image_path):\n",
        "            try:\n",
        "                caption = generate_caption(image_path, encoder, decoder, vocab, transform, device)\n",
        "                print(f\"Image {i+1} ({image_path}): {caption}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {image_path}: {e}\")\n",
        "        else:\n",
        "            print(f\"Image not found: {image_path}\")\n",
        "\n",
        "\n",
        "test_with_uploaded_images()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8MTzhKMaL1W1",
        "outputId": "b20e2d36-28fa-4f19-a4c2-b8e897568306"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5f6a707d-e73d-4154-b09c-0d7bcbdcd1d0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5f6a707d-e73d-4154-b09c-0d7bcbdcd1d0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pretrained_model.zip to pretrained_model.zip\n",
            "Saving vocap.zip to vocap.zip\n",
            "vocab.pkl: File Exists!\n",
            "encoder-5-3000.pkl: File Exists!\n",
            "decoder-5-3000.pkl: File Exists!\n",
            "Image Captioning with Pre-trained PyTorch Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100%|██████████| 230M/230M [00:03<00:00, 69.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded encoder weights from encoder-5-3000.pkl\n",
            "✓ Loaded decoder weights from decoder-5-3000.pkl\n",
            "✓ Models loaded successfully on cpu\n",
            "✓ Vocabulary size: 9956\n",
            "\\nGenerating captions for test images:\n",
            "\\nProcessing Image 1...\n",
            "URL: https://images.unsplash.com/photo-1552053831-71594a27632d?w=400\n",
            "Generated Caption: <start> a dog is sitting on a couch with a frisbee . <end>\n",
            "\\nProcessing Image 2...\n",
            "URL: https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400\n",
            "Generated Caption: <start> a cat is sitting on a couch with a remote . <end>\n",
            "\\nProcessing Image 3...\n",
            "URL: https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=400\n",
            "Generated Caption: <start> a man with a beard and a tie in his mouth . <end>\n",
            "\\nProcessing Image 4...\n",
            "URL: https://images.unsplash.com/photo-1549298916-b41d501d3772?w=400\n",
            "Generated Caption: <start> a cat laying on a bed next to a keyboard . <end>\n",
            "\\nProcessing Image 5...\n",
            "URL: https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400\n",
            "Generated Caption: <start> a mountain with a mountain in the background <end>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ae2aeb9a-e67e-4f96-a8f3-5abb76b87102\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ae2aeb9a-e67e-4f96-a8f3-5abb76b87102\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pexels-dariuskrs-2253938.jpg to pexels-dariuskrs-2253938.jpg\n",
            "Saving pexels-jamphotography-2626665.jpg to pexels-jamphotography-2626665.jpg\n",
            "Saving pexels-laker-6156582.jpg to pexels-laker-6156582.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded encoder weights from encoder-5-3000.pkl\n",
            "✓ Loaded decoder weights from decoder-5-3000.pkl\n",
            "Testing with my own uploaded images:\n",
            "Image 1 (/content/drive/MyDrive/Seq2SeqProject/pexels-laker-6156582.jpg): <start> a red and white bus parked in a parking lot . <end>\n",
            "Image 2 (/content/drive/MyDrive/Seq2SeqProject/pexels-dariuskrs-2253938.jpg): <start> a traffic light with a sign on top of it . <end>\n",
            "Image 3 (/content/drive/MyDrive/Seq2SeqProject/pexels-jamphotography-2626665.jpg): <start> a motorcycle parked on a street next to a building . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 3: Complete Working Solution (with vocab loading)\n",
        "\"\"\"\n",
        "    Transforming a PyTorch image captioning model into Keras has a number of steps which include translation of architecture, mapping of weights and adaptation of preprocessing.\n",
        "    As such, this conversion process would include:\n",
        "    Step 1. Architecture Analysis\n",
        "    Clearly, we can see that the PyTorch model structure utilizes:\n",
        "        - Encoder: feature extraction CNN (ResNet)\n",
        "        - Decoder: LSTM to generate sequences\n",
        "        - Embedding: for vocabulary embedding\n",
        "        - Linear Layers: for projecting output\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Step 2. Implementing Keras Architecture\n",
        "\"\"\"\n",
        "      - In this step we will implement replicate EncoderCNN and DecoderRNN with keras.\n",
        "\"\"\"\n",
        "\n",
        "# First we will load the vocabulary to get vocab_size\n",
        "with open('vocab.pkl', 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Loaded vocabulary with {vocab_size} tokens\")\n",
        "\n",
        "# Second, we our trained PyTorch models\n",
        "pt_encoder = torch.load('encoder-5-3000.pkl', map_location='cpu')\n",
        "pt_decoder = torch.load('decoder-5-3000.pkl', map_location='cpu')\n",
        "\n",
        "#Then we will create Keras equivalent encoder\n",
        "\"\"\"We will use ResNet152.\"\"\"\n",
        "\n",
        "def create_keras_encoder():\n",
        "    base_model = tf.keras.applications.ResNet152(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(224, 224, 3)\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "\n",
        "    inputs = Input(shape=(224, 224, 3))\n",
        "    x = base_model(inputs)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, name='dense')(x)  # embed_size=256\n",
        "    x = tf.keras.layers.BatchNormalization(momentum=0.01, name='bn')(x)\n",
        "\n",
        "    return Model(inputs, x)\n",
        "\n",
        "#Then we will create Keras equivalent encoder\n",
        "\"\"\" We will use keras LSTM.\"\"\"\n",
        "def create_keras_decoder(vocab_size):\n",
        "    # Inputs\n",
        "    features = Input(shape=(256,))  # embed_size\n",
        "    sequence = Input(shape=(None,))\n",
        "\n",
        "    # Embedding\n",
        "    embedding = Embedding(vocab_size, 256)(sequence)\n",
        "\n",
        "    # LSTM (hidden_size=512)\n",
        "    lstm = LSTM(512, return_sequences=True, name='lstm')\n",
        "    lstm_out = lstm(embedding, initial_state=[\n",
        "        Dense(512)(features),  # h_0\n",
        "        Dense(512)(features)   # c_0\n",
        "    ])\n",
        "\n",
        "    # Output\n",
        "    outputs = Dense(vocab_size, activation='softmax')(lstm_out)\n",
        "\n",
        "    return Model([features, sequence], outputs)\n",
        "\n",
        "# Step 3. Weight Conversion Process\n",
        "\"\"\"\n",
        "    - We can accomplish this process through three main tactics, including:\n",
        "\n",
        "      1. Weight Transfer\n",
        "      2. Using ONNX as Intermediate Format\n",
        "      3. Retraining afresh in Keras\n",
        "\n",
        "    - But since we want to be able to use the learned weights for PyTorch in Keras, we will have to take the first method as our best approach.\n",
        "\n",
        "    - Although ONNX has general-purpose conversion path, manual weight transfer allows complete numerical preservation of all parameters learned with PyTorch and transferring them to Keras.\n",
        "\n",
        "    - This is essential to sequencing applications where oftentimes minor numerical errors can propagate to totally distinct outcomes.\n",
        "\n",
        "    - There are PyTorch patterns that are used in our image captioning model that ONNX would typically have trouble converting which are (1) teacher forcing during training (2) stateful sampling during inference.\n",
        "\n",
        "    - As such, weight transfer allows for exact duplicate LSTM initialization of state, retaining the precise mapping of vocabulary in the embedding layer and the use of exact batch normalization momentum (0.01) as the one set in PyTorch.\n",
        "\n",
        "\"\"\"\n",
        "def transfer_encoder_weights(pt_encoder, keras_encoder):\n",
        "    # Dense layer\n",
        "    keras_encoder.get_layer('dense').set_weights([\n",
        "        pt_encoder['linear.weight'].numpy().T,  # Transpose for Keras\n",
        "        pt_encoder['linear.bias'].numpy()\n",
        "    ])\n",
        "\n",
        "    # BatchNorm\n",
        "    keras_encoder.get_layer('bn').set_weights([\n",
        "        pt_encoder['bn.weight'].numpy(),\n",
        "        pt_encoder['bn.bias'].numpy(),\n",
        "        pt_encoder['bn.running_mean'].numpy(),\n",
        "        pt_encoder['bn.running_var'].numpy()\n",
        "    ])\n",
        "\n",
        "def transfer_decoder_weights(pt_decoder, keras_decoder):\n",
        "    \"\"\"Fixed version handling automatic layer naming\"\"\"\n",
        "    # 1. Finding the embedding layer by type (more robust than name)\n",
        "    embedding_layer = None\n",
        "    for layer in keras_decoder.layers:\n",
        "        if isinstance(layer, Embedding):\n",
        "            embedding_layer = layer\n",
        "            break\n",
        "\n",
        "    if embedding_layer is None:\n",
        "        raise ValueError(\"No Embedding layer found in decoder\")\n",
        "\n",
        "    # 2. Transfering embedding weights\n",
        "    embedding_layer.set_weights([pt_decoder['embed.weight'].numpy()])\n",
        "\n",
        "    # 3. Transfering LSTM weights (with gate reordering)\n",
        "    W_ih = pt_decoder['lstm.weight_ih_l0'].numpy()  # (4*hidden, embed)\n",
        "    W_hh = pt_decoder['lstm.weight_hh_l0'].numpy()  # (4*hidden, hidden)\n",
        "    bias = (pt_decoder['lstm.bias_ih_l0'] + pt_decoder['lstm.bias_hh_l0']).numpy()\n",
        "\n",
        "    # Reordering gates: PyTorch (i,f,g,o) → Keras (i,f,o,g)\n",
        "    W_i, W_f, W_g, W_o = np.split(W_ih, 4)\n",
        "    U_i, U_f, U_g, U_o = np.split(W_hh, 4)\n",
        "    b_i, b_f, b_g, b_o = np.split(bias, 4)\n",
        "\n",
        "    lstm_layer = keras_decoder.get_layer('lstm')  # Using explicit name\n",
        "    lstm_layer.set_weights([\n",
        "        np.concatenate([W_i, W_f, W_o, W_g]).T,\n",
        "        np.concatenate([U_i, U_f, U_o, U_g]).T,\n",
        "        np.concatenate([b_i, b_f, b_o, b_g])\n",
        "    ])\n",
        "\n",
        "    # 4. Transfering output layer weights\n",
        "    output_layer = keras_decoder.layers[-1]  # Last layer is Dense\n",
        "    output_layer.set_weights([\n",
        "        pt_decoder['linear.weight'].numpy().T,\n",
        "        pt_decoder['linear.bias'].numpy()\n",
        "    ])\n",
        "\n",
        "# 5. Initializing and transfer weights\n",
        "keras_encoder = create_keras_encoder()\n",
        "keras_decoder = create_keras_decoder(vocab_size)\n",
        "\n",
        "transfer_encoder_weights(pt_encoder, keras_encoder)\n",
        "transfer_decoder_weights(pt_decoder, keras_decoder)\n",
        "\n",
        "# 6. Saving models\n",
        "keras_encoder.save('keras_encoder.h5')\n",
        "keras_decoder.save('keras_decoder.h5')\n",
        "\n",
        "print(\"Model converted to Keras successfully!\")\n",
        "\n",
        "# Step 4. Inference Implementation\n",
        "\"\"\" -In this step, we implement Inference.\n",
        "    -This is how the trained model can be used to produce captions to new images- without changing any weights.\n",
        "\"\"\"\n",
        "def generate_caption(image_path, max_length=20):\n",
        "    # Preprocessing image (Keras-style)\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img = tf.keras.applications.resnet.preprocess_input(img[np.newaxis, ...])\n",
        "\n",
        "    # Getting features\n",
        "    features = keras_encoder.predict(img)\n",
        "\n",
        "    # Initialize sequence\n",
        "    seq = [vocab['<start>']]\n",
        "\n",
        "    # Generating caption\n",
        "    for _ in range(max_length):\n",
        "        # Preparing input\n",
        "        seq_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [seq], maxlen=max_length, padding='post'\n",
        "        )\n",
        "\n",
        "        # Predicting next word\n",
        "        preds = keras_decoder.predict([features, seq_input], verbose=0)\n",
        "        next_id = np.argmax(preds[0, len(seq)-1])\n",
        "        seq.append(next_id)\n",
        "\n",
        "        if next_id == vocab['<end>']:\n",
        "            break\n",
        "\n",
        "    # Converting to text\n",
        "    return ' '.join([vocab.idx2word[i] for i in seq[1:-1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXpzUK76PjFt",
        "outputId": "25d2d63e-792a-4fe8-c5d4-f47a681c9bc7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded vocabulary with 9956 tokens\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m234698864/234698864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model converted to Keras successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 4: (Advance assignment) Code reading and rewriting\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Concatenate, GlobalAveragePooling2D, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_image_captioning_model(vocab_size, embed_size=256, hidden_size=512, max_length=20):\n",
        "    \"\"\"\n",
        "    Keras model equivalent to model.py (PyTorch EncoderCNN + DecoderRNN).\n",
        "    Uses machine translation seq2seq style as reference.\n",
        "    \"\"\"\n",
        "\n",
        "    # Encoder: Image feature extractor\n",
        "    image_input = Input(shape=(224, 224, 3), name='image_input')\n",
        "    base_model = tf.keras.applications.ResNet152(include_top=False, weights='imagenet')\n",
        "    base_model.trainable = False\n",
        "    x = base_model(image_input)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    image_features = Dense(embed_size, activation='relu', name='image_features')(x)\n",
        "\n",
        "    # Decoder: Caption sequence processor\n",
        "    caption_input = Input(shape=(max_length,), name='caption_input')\n",
        "    caption_embedding = Embedding(input_dim=vocab_size, output_dim=embed_size, mask_zero=True, name='word_embedding')(caption_input)\n",
        "\n",
        "    # Reshaping image features so they behave like one token\n",
        "    image_features_expanded = Reshape((1, embed_size))(image_features)\n",
        "\n",
        "    # Combining image features with caption embeddings\n",
        "    decoder_input = Concatenate(axis=1)([image_features_expanded, caption_embedding])\n",
        "\n",
        "    lstm_out = LSTM(hidden_size, return_sequences=True, name='decoder_lstm')(decoder_input)\n",
        "    output = Dense(vocab_size, activation='softmax', name='output_word')(lstm_out)\n",
        "\n",
        "    model = Model(inputs=[image_input, caption_input], outputs=output, name=\"ImageCaptioningModel\")\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "vocab_size = 10000  # hypothetical\n",
        "max_length = 20\n",
        "model = build_image_captioning_model(vocab_size, max_length=max_length)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "rilIKxaXdADZ",
        "outputId": "174f61e5-96fe-4f49-f28b-3927e47b2e8f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"ImageCaptioningModel\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ImageCaptioningModel\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ resnet152           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      │ \u001b[38;5;34m58,370,944\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ resnet152[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ caption_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ image_features      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m524,544\u001b[0m │ global_average_p… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ caption_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ image_features[\u001b[38;5;34m0\u001b[0m… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ expand_dims         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mExpandDims\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ones_like           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mOnesLike\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ broadcast_to        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ expand_dims[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mBroadcastTo\u001b[0m)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ word_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m2,560,000\u001b[0m │ caption_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ ones_like[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ broadcast_to[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ word_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ any (\u001b[38;5;33mAny\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,574,912\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_word (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m10000\u001b[0m) │  \u001b[38;5;34m5,130,000\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ resnet152           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">58,370,944</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ resnet152[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ caption_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ image_features      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ global_average_p… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ caption_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ expand_dims         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ExpandDims</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ones_like           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OnesLike</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ broadcast_to        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ expand_dims[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BroadcastTo</span>)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ word_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ caption_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ones_like[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ broadcast_to[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ any (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Any</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_word (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130,000</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m68,160,400\u001b[0m (260.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,160,400</span> (260.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,789,456\u001b[0m (37.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,789,456</span> (37.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m58,370,944\u001b[0m (222.67 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,370,944</span> (222.67 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 5: (Advance assignment) Developmental survey**\n",
        "\n",
        "**A. When translating into other languages**\n",
        "\n",
        "To implement additional languages (including French, German, or Swahili) with Problem 1 that translates in between English and Japanese, it will be necessary to take planning very seriously.\n",
        "\n",
        "Data preparation would be the first consideration and in this preparation, the parallel corpora should be gathered that includes the sentence pairs of the intended language pairs. Examples of these corpora are `Europarl` (European languages), `UN Corpus` (multi-language corpora), and `JW300` (low-resource language corpus). Upon the collection of the data, there are text normalization and tokenization in accordance with each language that should be done so that they are consistent.\n",
        "\n",
        "Tokenizer and vocabulary design is another important procedure in multiple language handling. Other common approaches to tokenization include `subword tokenization` where large vocabularies with rare words are addressed more effectively using `Byte-Pair Encoding (BPE)` or `SentencePiece`. The use of tokenizers will depend on the complexity of the task: each tokenizer can be trained individually between language pairs or incorporated into one shared tokenizer in multi-language models.\n",
        "\n",
        "Along with it is model adaptation. It can be done by extending encoder to decoder systems to support several languages. The first one would be applying the construction of one multilingual model based on special language tokens like`<2fr>` to pinpoint the target language. In a different setup, it is possible to conduct training of individual language pairs, and, in this case, it is simpler to train but more costly in computation.\n",
        "\n",
        "Incorporation through transfer learning is possible by using `mBART, MarianMT, or mT5`, all of which are trained with the intention to apply to many languages using pretrained multilingual settings. The efficiency of adapting even in low-resource settings will occur as a result of fine-tuning of these models with new language pairs.\n",
        "\n",
        "Lastly, there is assessment to be taken into consideration. Quality of a translation can be measured automatically using `BLEU, METEOR, and chrF++`, however, in many cases, IP quality should be evaluated by a person, which is because translation requires a person to understand the context and take into account linguistic peculiarities.\n",
        "\n",
        "**B. What are the advanced methods of machine translation**\n",
        "\n",
        "Machine translation has tremendously changed from past statistical algorithms.\n",
        "\n",
        "`Neural Machine Translation (NMT)` is a key breakthrough, as it employed the sequence-to-sequence models, and attention mechanisms (Sequences), like `Bahdanau` or `Luong attention`, which enhanced context management.\n",
        "\n",
        "The next big innovation is `transformer-based` architectures, which have popularized the self-attention mechanism, which allows `Google Transformer`,` OpenNMT` and `Fairseq` to perform at a higher level of accuracy and support parallel training.\n",
        "\n",
        "The more recent development is the `pretrained multilingual models`. Multilingual models like `mBART`, `MarianMT`, or `M2M-100` are trained on tens or even hundreds of languages jointly and so do not require direct exposure to pairs of languages to translate between them, so-called zero-shot translation.\n",
        "\n",
        "Furthermore, `document-level translation` methods in `Context-Aware Translation` have also enhance context discovery, by taking into account whole paragraphs or documents, rather than individual sentences, overcoming such issues as pronoun resolution and style coherence.\n",
        "\n",
        "`Transfer learning` in `low-resource language translation` has been useful in the transfer of knowledge in high-resource languages to enhance performance in low-resource languages. What is more, `unsupervised NMT` methods have been demonstrated to work even on monolingual data in the absence of parallel data.\n",
        "\n",
        "**C. How to generate an image from text**\n",
        "\n",
        "Whereas image captioning is a method of translating a visual input into text, text-to-image generation goes the other way around. Historically, this was done either with retrieval based systems that retrieved existing images that resembled the text description or in the case of early `Generative Adversarial Networks (GANs)`, they generated simplistic images given some text embeddings.\n",
        "\n",
        "New methods have yielded great results and modern methods such as GAN-based models like `StackGAN` and `AttnGAN` have gone further to apply attention mechanisms to enable the model to produce images of finer detail and which match textual descriptions more closely. Nevertheless, the best techniques are now resting on diffusion models, such as `DALL.E 2`, `Stable Diffusion`, and `MidJourney`. Such models are designed to learn to progressively clean random noise into a coherent image given a text prompt, and therefore, can be used to produce high-resolution, photorealistic, and highly creative images.\n",
        "\n",
        "Text-to-image generation dependency is based on the natural language understanding based either on large language model embedding or vision-language models such as `CLIP` to embed text meaning into the visual world. The examples of use of these models are concept art and product design, educational visualizations, and marketing materials. Nevertheless, there are still obstacles, such as possible biases in the produced material, the inability to manage fine-grained details of pictures, including style, structure, etc., and the fact that they are computer-demanding to train and infer."
      ],
      "metadata": {
        "id": "jxhVrrymg6ww"
      }
    }
  ]
}