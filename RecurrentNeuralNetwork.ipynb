{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvUACuP8iCLhI5fN4jLA1+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Odima-dev/Data-Science-and-Machine-Learning/blob/main/RecurrentNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 1: Simple Forward propagation implementation of RNN\n",
        "class SimpleRNN:\n",
        "    \"\"\"\n",
        "    Simple RNN layer implementation from scratch\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes, initializer='he', optimizer=None):\n",
        "        \"\"\"\n",
        "        Initialize SimpleRNN layer\n",
        "\n",
        "        Parameters:\n",
        "        n_nodes: int, number of RNN nodes\n",
        "        initializer: str, weight initialization method\n",
        "        optimizer: optimizer object (not used in this basic implementation)\n",
        "        \"\"\"\n",
        "        self.n_nodes = n_nodes\n",
        "        self.initializer = initializer\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Weights and biases will be initialized in _initialize_weights\n",
        "        self.W_x = None  # Input weights\n",
        "        self.W_h = None  # Hidden weights\n",
        "        self.B = None    # Bias\n",
        "\n",
        "        # For storing values during forward propagation (needed for backprop)\n",
        "        self.h_states = []  # This will store all hidden states\n",
        "        self.a_states = []  # Stores all pre-activation states\n",
        "        self.x_inputs = []  # Will store all inputs\n",
        "\n",
        "    def _initialize_weights(self, n_features):\n",
        "        \"\"\"Initialize weights and biases\"\"\"\n",
        "        if self.initializer == 'he':\n",
        "            # He initialization\n",
        "            self.W_x = np.random.randn(n_features, self.n_nodes) * np.sqrt(2.0 / n_features)\n",
        "            self.W_h = np.random.randn(self.n_nodes, self.n_nodes) * np.sqrt(2.0 / self.n_nodes)\n",
        "        else:\n",
        "            # Simple random initialization\n",
        "            self.W_x = np.random.randn(n_features, self.n_nodes) * 0.01\n",
        "            self.W_h = np.random.randn(self.n_nodes, self.n_nodes) * 0.01\n",
        "\n",
        "        self.B = np.zeros(self.n_nodes)\n",
        "\n",
        "    def forward(self, X, h_prev=None):\n",
        "        \"\"\"\n",
        "        Forward propagation\n",
        "\n",
        "        Parameters:\n",
        "        X: ndarray, shape (batch_size, n_sequences, n_features)\n",
        "        h_prev: ndarray, shape (batch_size, n_nodes), initial hidden state\n",
        "\n",
        "        Returns:\n",
        "        h_final: ndarray, shape (batch_size, n_nodes), final hidden state\n",
        "        \"\"\"\n",
        "        batch_size, n_sequences, n_features = X.shape\n",
        "\n",
        "        # Initializing weights if not already done\n",
        "        if self.W_x is None:\n",
        "            self._initialize_weights(n_features)\n",
        "\n",
        "        # Initializing hidden state if not provided\n",
        "        if h_prev is None:\n",
        "            h_prev = np.zeros((batch_size, self.n_nodes))\n",
        "\n",
        "        # Clearing previous stored states\n",
        "        self.h_states = []\n",
        "        self.a_states = []\n",
        "        self.x_inputs = []\n",
        "\n",
        "        # Storing initial hidden state\n",
        "        self.h_states.append(h_prev)\n",
        "\n",
        "        h_t = h_prev\n",
        "\n",
        "        # Processing each time step\n",
        "        for t in range(n_sequences):\n",
        "            x_t = X[:, t, :]  # Input at time t: (batch_size, n_features)\n",
        "\n",
        "            # Storing input for backpropagation\n",
        "            self.x_inputs.append(x_t)\n",
        "\n",
        "            # Computing pre-activation: a_t = x_t @ W_x + h_{t-1} @ W_h + B\n",
        "            a_t = np.dot(x_t, self.W_x) + np.dot(h_t, self.W_h) + self.B\n",
        "\n",
        "            # Storing pre-activation for backpropagation\n",
        "            self.a_states.append(a_t)\n",
        "\n",
        "            # Applying activation function: h_t = tanh(a_t)\n",
        "            h_t = np.tanh(a_t)\n",
        "\n",
        "            # Storing hidden state for backpropagation\n",
        "            self.h_states.append(h_t)\n",
        "\n",
        "        return h_t\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        \"\"\"\n",
        "        Backward propagation\n",
        "\n",
        "        Parameters:\n",
        "        dh_next: ndarray, shape (batch_size, n_nodes), gradient from next layer\n",
        "\n",
        "        Returns:\n",
        "        dX: ndarray, shape (batch_size, n_sequences, n_features), gradient w.r.t. input\n",
        "        \"\"\"\n",
        "        batch_size = dh_next.shape[0]\n",
        "        n_sequences = len(self.x_inputs)\n",
        "        n_features = self.x_inputs[0].shape[1]\n",
        "\n",
        "        # Initializing gradients\n",
        "        dW_x = np.zeros_like(self.W_x)\n",
        "        dW_h = np.zeros_like(self.W_h)\n",
        "        dB = np.zeros_like(self.B)\n",
        "        dX = np.zeros((batch_size, n_sequences, n_features))\n",
        "\n",
        "        # Initializing gradient flowing back through time\n",
        "        dh_t = dh_next\n",
        "\n",
        "        # Backpropagating through time (from last to first time step)\n",
        "        for t in reversed(range(n_sequences)):\n",
        "            # Getting stored values\n",
        "            x_t = self.x_inputs[t]\n",
        "            a_t = self.a_states[t]\n",
        "            h_prev = self.h_states[t]  # h_{t-1}\n",
        "\n",
        "            # Gradient of tanh activation: d_tanh/da = 1 - tanh^2(a)\n",
        "            dtanh_da = 1 - np.tanh(a_t) ** 2\n",
        "\n",
        "            # Gradient w.r.t. pre-activation: da_t = dh_t * dtanh_da\n",
        "            da_t = dh_t * dtanh_da\n",
        "\n",
        "            # Accumulating gradients for weights and bias\n",
        "            dW_x += np.dot(x_t.T, da_t)\n",
        "            dW_h += np.dot(h_prev.T, da_t)\n",
        "            dB += np.sum(da_t, axis=0)\n",
        "\n",
        "            # Gradient w.r.t. input at time t\n",
        "            dX[:, t, :] = np.dot(da_t, self.W_x.T)\n",
        "\n",
        "            # Gradient w.r.t. previous hidden state (flows back through time)\n",
        "            dh_t = np.dot(da_t, self.W_h.T)\n",
        "\n",
        "        # Storing gradients for weight updates\n",
        "        self.dW_x = dW_x\n",
        "        self.dW_h = dW_h\n",
        "        self.dB = dB\n",
        "\n",
        "        return dX\n",
        "\n",
        "    def update_weights(self, learning_rate):\n",
        "        \"\"\"Update weights using gradients\"\"\"\n",
        "        self.W_x -= learning_rate * self.dW_x\n",
        "        self.W_h -= learning_rate * self.dW_h\n",
        "        self.B -= learning_rate * self.dB"
      ],
      "metadata": {
        "id": "6TGPlwqulw90"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2: Experiment of forward propagation with small sequence\n",
        "x = np.array([[[1, 2], [2, 3], [3, 4]]]) / 100  # (batch_size, n_sequences, n_features)\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]]) / 100  # (n_features, n_nodes)\n",
        "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]]) / 100  # (n_nodes, n_nodes)\n",
        "\n",
        "batch_size = x.shape[0]  # 1\n",
        "n_sequences = x.shape[1]  # 3\n",
        "n_features = x.shape[2]  # 2\n",
        "n_nodes = w_x.shape[1]  # 4\n",
        "h = np.zeros((batch_size, n_nodes))  # (batch_size, n_nodes)\n",
        "b = np.array([1, 1, 1, 1])  # (n_nodes,)\n",
        "\n",
        "# Creating RNN layer and set the specified weights\n",
        "rnn = SimpleRNN(n_nodes)\n",
        "rnn.W_x = w_x\n",
        "rnn.W_h = w_h\n",
        "rnn.B = b\n",
        "\n",
        "# Testing forward propagation\n",
        "\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Input values:\")\n",
        "print(x)\n",
        "\n",
        "# Checking if result matches expected output\n",
        "result = rnn.forward(x, h)\n",
        "print(\"\\nFinal hidden state:\", result)\n",
        "expected = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])\n",
        "print(\"\\nExpected output:\", expected)\n",
        "\n",
        "print(\"\\nCode output of forward propagation matches the expected code\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5aYX9ohmyS6",
        "outputId": "f7e4d10b-d62e-497b-d2e5-deb8bf3edb6d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (1, 3, 2)\n",
            "Input values:\n",
            "[[[0.01 0.02]\n",
            "  [0.02 0.03]\n",
            "  [0.03 0.04]]]\n",
            "\n",
            "Final hidden state: [[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
            "\n",
            "Expected output: [[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
            "\n",
            "Code output of forward propagation matches the expected code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 3: (Advance assignment) Implementation of backpropagation\n",
        "class ScratchSimpleRNNClassifier:\n",
        "    \"\"\"\n",
        "    Simple RNN Classifier implementation from scratch\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes=10, learning_rate=0.01, n_epochs=100):\n",
        "        \"\"\"\n",
        "        Initialize RNN classifier\n",
        "\n",
        "        Parameters:\n",
        "        n_nodes: int, number of RNN nodes\n",
        "        learning_rate: float, learning rate for optimization\n",
        "        n_epochs: int, number of training epochs\n",
        "        \"\"\"\n",
        "        self.n_nodes = n_nodes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_epochs = n_epochs\n",
        "\n",
        "        # Layers will be initialized during fit\n",
        "        self.rnn_layer = None\n",
        "        self.output_layer = None\n",
        "\n",
        "    def _softmax(self, x):\n",
        "        \"\"\"Softmax activation function\"\"\"\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def _cross_entropy_loss(self, y_true, y_pred):\n",
        "        \"\"\"Cross entropy loss function\"\"\"\n",
        "        n_samples = y_true.shape[0]\n",
        "        return -np.sum(y_true * np.log(y_pred + 1e-8)) / n_samples\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the RNN classifier\n",
        "\n",
        "        Parameters:\n",
        "        X: ndarray, shape (n_samples, n_sequences, n_features)\n",
        "        y: ndarray, shape (n_samples, n_classes) - one-hot encoded\n",
        "        \"\"\"\n",
        "        n_samples, n_sequences, n_features = X.shape\n",
        "        n_classes = y.shape[1]\n",
        "\n",
        "        # Initializing layers\n",
        "        self.rnn_layer = SimpleRNN(self.n_nodes)\n",
        "\n",
        "        # Initializing output layer weights\n",
        "        self.W_out = np.random.randn(self.n_nodes, n_classes) * 0.01\n",
        "        self.b_out = np.zeros(n_classes)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(self.n_epochs):\n",
        "            # Forward propagation\n",
        "            h_final = self.rnn_layer.forward(X)\n",
        "\n",
        "            # Output layer forward propagation\n",
        "            z_out = np.dot(h_final, self.W_out) + self.b_out\n",
        "            y_pred = self._softmax(z_out)\n",
        "\n",
        "            # Computing loss\n",
        "            loss = self._cross_entropy_loss(y, y_pred)\n",
        "\n",
        "            # Backward propagation\n",
        "            # Gradient w.r.t. output layer\n",
        "            dz_out = y_pred - y\n",
        "            dW_out = np.dot(h_final.T, dz_out) / n_samples\n",
        "            db_out = np.sum(dz_out, axis=0) / n_samples\n",
        "\n",
        "            # Gradient w.r.t. final hidden state\n",
        "            dh_final = np.dot(dz_out, self.W_out.T)\n",
        "\n",
        "            # RNN backward propagation\n",
        "            self.rnn_layer.backward(dh_final)\n",
        "\n",
        "            # Updating weights\n",
        "            self.rnn_layer.update_weights(self.learning_rate)\n",
        "            self.W_out -= self.learning_rate * dW_out\n",
        "            self.b_out -= self.learning_rate * db_out\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions\n",
        "\n",
        "        Parameters:\n",
        "        X: ndarray, shape (n_samples, n_sequences, n_features)\n",
        "\n",
        "        Returns:\n",
        "        predictions: ndarray, shape (n_samples,)\n",
        "        \"\"\"\n",
        "        h_final = self.rnn_layer.forward(X)\n",
        "        z_out = np.dot(h_final, self.W_out) + self.b_out\n",
        "        y_pred = self._softmax(z_out)\n",
        "        return np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Demonstrating the implementation for Problem 3 Above\n",
        "# Creating synthetic data for classification\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "n_sequences = 5\n",
        "n_features = 3\n",
        "n_classes = 2\n",
        "\n",
        "# Generating synthetic sequence data\n",
        "X_demo = np.random.randn(n_samples, n_sequences, n_features)\n",
        "# Creating labels based on sum of sequence\n",
        "y_demo_labels = (np.sum(X_demo, axis=(1, 2)) > 0).astype(int)\n",
        "# Converting to one-hot encoding\n",
        "y_demo = np.eye(n_classes)[y_demo_labels]\n",
        "\n",
        "print(f\"Demo data shape: {X_demo.shape}\")\n",
        "print(f\"Demo labels shape: {y_demo.shape}\")\n",
        "\n",
        "# Creating and training classifier\n",
        "classifier = ScratchSimpleRNNClassifier(n_nodes=8, learning_rate=0.1, n_epochs=50)\n",
        "print(\"\\nTraining classifier...\")\n",
        "classifier.fit(X_demo, y_demo)\n",
        "\n",
        "# Making predictions\n",
        "predictions = classifier.predict(X_demo)\n",
        "accuracy = np.mean(predictions == y_demo_labels)\n",
        "print(f\"\\nTraining accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkDX76apnOMK",
        "outputId": "f2c74fd7-d52f-4d2a-e976-43fe9e36ee8f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demo data shape: (100, 5, 3)\n",
            "Demo labels shape: (100, 2)\n",
            "\n",
            "Training classifier...\n",
            "Epoch 0, Loss: 0.6943\n",
            "Epoch 10, Loss: 0.3533\n",
            "Epoch 20, Loss: 0.3998\n",
            "Epoch 30, Loss: 0.2501\n",
            "Epoch 40, Loss: 0.1191\n",
            "\n",
            "Training accuracy: 0.9400\n"
          ]
        }
      ]
    }
  ]
}